# -*- coding: utf-8 -*-
"""Foursquare API.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VT6sHV9C22Cjo28R_bnZcVUv_B9paNBN
"""

import requests # library to handle requests
import pandas as pd # library for data analsysis
import numpy as np # library to handle data in a vectorized manner
import random # library for random number generation

!conda install -c conda-forge geopy --yes 
from geopy.geocoders import Nominatim # module to convert an address into latitude and longitude values

# libraries for displaying images
from IPython.display import Image 
from IPython.core.display import HTML 
    
# tranforming json file into a pandas dataframe library
from pandas.io.json import json_normalize

!conda install -c conda-forge folium=0.5.0 --yes
import folium # plotting library

print('Folium installed')
print('Libraries imported.')

CLIENT_ID = '2F1CJNJUWG2YWCD4UCXMQ4UJ4YB3K21Z4AIAHWRXJX02EJY2' # your Foursquare ID
CLIENT_SECRET = 'A24ALYYVZVBLHWB2C52MEU5GXPVFMGT40HF1C30K1J3DYSL0' # your Foursquare Secret
ACCESS_TOKEN = '' # your FourSquare Access Token
VERSION = '20180604'
LIMIT = 30
print('Your credentails:')
print('CLIENT_ID: ' + CLIENT_ID)
print('CLIENT_SECRET:' + CLIENT_SECRET)

address = 'Soho Hotel,London,United Kingdom'

geolocator = Nominatim(user_agent="foursquare_agent")
location = geolocator.geocode(address)
latitude = location.latitude
longitude = location.longitude
print(latitude, longitude)

search_query = 'Cafe'
radius = 500
print(search_query + ' .... OK!')

url = 'https://api.foursquare.com/v2/venues/search?client_id={}&client_secret={}&ll={},{}&v={}&query={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, search_query, radius, LIMIT)

url

results = requests.get(url).json()
results



# assign relevant part of JSON to venues
venues = results['response']['venues']

# tranform venues into a dataframe
dataframe = json_normalize(venues)
dataframe.head()

# keep only columns that include venue name, and anything that is associated with location
filtered_columns = ['name', 'categories'] + [col for col in dataframe.columns if col.startswith('location.')] + ['id']
dataframe_filtered = dataframe.loc[:, filtered_columns]

# function that extracts the category of the venue
def get_category_type(row):
    try:
        categories_list = row['categories']
    except:
        categories_list = row['venue.categories']
        
    if len(categories_list) == 0:
        return None
    else:
        return categories_list[0]['name']

# filter the category for each row
dataframe_filtered['categories'] = dataframe_filtered.apply(get_category_type, axis=1)

# clean column names by keeping only last term
dataframe_filtered.columns = [column.split('.')[-1] for column in dataframe_filtered.columns]

dataframe_filtered

dataframe_filtered.name

venues_map = folium.Map(location=[latitude, longitude], zoom_start=13) # generate map centred around the Conrad Hotel

# add a red circle marker to represent the Conrad Hotel
folium.CircleMarker(
    [latitude, longitude],
    radius=10,
    color='red',
    popup='Taj Hotel',
    fill = True,
    fill_color = 'red',
    fill_opacity = 0.6
).add_to(venues_map)

# add the Italian restaurants as blue circle markers
for lat, lng, label in zip(dataframe_filtered.lat, dataframe_filtered.lng, dataframe_filtered.categories):
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        color='blue',
        popup=label,
        fill = True,
        fill_color='blue',
        fill_opacity=0.6
    ).add_to(venues_map)

# display map
venues_map

latitude = 51.5033
longitude = 0.1196

url = 'https://api.foursquare.com/v2/venues/explore?client_id={}&client_secret={}&ll={},{}&v={}&radius={}&limit={}'.format(CLIENT_ID, CLIENT_SECRET, latitude, longitude, VERSION, radius, LIMIT)

url

import requests

results = requests.get(url).json()
'There are {} around the London Eye'.format(len(results['response']['groups'][0]['items']))

items = results['response']['groups'][0]['items']
items[0]

dataframe = json_normalize(items) # flatten JSON

# filter columns
filtered_columns = ['venue.name', 'venue.categories'] + [col for col in dataframe.columns if col.startswith('venue.location.')] + ['venue.id']
dataframe_filtered = dataframe.loc[:, filtered_columns]

# filter the category for each row
dataframe_filtered['venue.categories'] = dataframe_filtered.apply(get_category_type, axis=1)

# clean columns
dataframe_filtered.columns = [col.split('.')[-1] for col in dataframe_filtered.columns]

dataframe_filtered.head()

venues_map = folium.Map(location=[latitude, longitude], zoom_start=15) # generate map centred around Ecco


# add Ecco as a red circle mark
folium.CircleMarker(
    [latitude, longitude],
    radius=10,
    popup='Ecco',
    fill=True,
    color='red',
    fill_color='red',
    fill_opacity=0.6
    ).add_to(venues_map)


# add popular spots to the map as blue circle markers
for lat, lng, label in zip(dataframe_filtered.lat, dataframe_filtered.lng, dataframe_filtered.categories):
    folium.CircleMarker(
        [lat, lng],
        radius=5,
        popup=label,
        fill=True,
        color='blue',
        fill_color='blue',
        fill_opacity=0.6
        ).add_to(venues_map)

# display map
venues_map

"""https://pub.towardsai.net/web-scraping-yelp-part-3-performing-an-eda-on-yelp-scraped-data-176d38fee302[bağlantı metni](https://)"""

import requests
from bs4 import BeautifulSoup
import time
from textblob import TextBlob
import pandas as pd
#we use these argument to scrape the website
rest_dict = [
              {  "name"   : "the-cortez-raleigh",
                  "link"  : "https://www.yelp.com/biz/the-cortez-raleigh?osq=Restaurants&start=",
                  "pages" : 3
              },
              {  "name"   : "rosewater-kitchen-and-bar-raleigh",
                  "link"  : "https://www.yelp.com/biz/rosewater-kitchen-and-bar-raleigh?osq=Restaurants&start=",
                  "pages" : 3
              }
]
#scraping function
def scrape(rest_list):
  all_comment_list = list()
  for rest in rest_list:
    comment_list = list()
    for pag in range(1, rest['pages']):
      try:
        time.sleep(5)
#URL = "https://www.yelp.com/biz/the-cortez-raleigh?osq=Restaurants&start="+str(pag*10)+"&sort_by=rating_asc"
        URL = rest['link']+str(pag*10)
        print(rest['name'], 'downloading page ', pag*10)
        page = requests.get(URL)
#next step: parsing
        soup = BeautifulSoup(page.content, 'lxml')
        soup
        for comm in soup.find("yelp-react-root").find_all("p", {"class" : "comment__373c0__Nsutg css-n6i4z7"}):
          comment_list.append(comm.find("span").decode_contents())
          print(comm.find("span").decode_contents())
      except:
        print("could not work properly!")
    all_comment_list.append([comment_list, rest['name']])
  return all_comment_list
#store all reviews in a list
reviews = scrape(rest_dict)

import requests
from bs4 import BeautifulSoup
import time
from textblob import TextBlob
import pandas as pd
#we use these argument to scrape the website
rest_dict = [
              {  "name"   : "Hard Rock Cafe,Piccadily Circus",
                  "link"  : "https://www.yelp.com/biz/hard-rock-cafe-london-3?osq=Hard+Rock+Cafe+Piccadilly+Circus",
                  "pages" : 3
              },
              {  "name"   : "Rainforest Cafe,Soho",
                  "link"  : "https://www.yelp.com/biz/the-rainforest-cafe-london?osq=Hard+Rock+Cafe+Piccadilly+Circus",
                  "pages" : 3
              },
             {  "name"   : "Planet Hollywood",
                  "link"  : "https://www.yelp.com/biz/planet-hollywood-london-2?osq=Hard+Rock+Cafe+Piccadilly+Circus",
                  "pages" : 3
              },
             {  "name"   : "Honest Burgers",
                  "link"  : "https://www.yelp.com/biz/honest-burgers-meard-st-soho-london?osq=Hard+Rock+Cafe+Piccadilly+Circus",
                  "pages" : 3
              },
             {  "name"   : "Toi and Moi Cafe",
                  "link"  : "https://www.yelp.com/biz/toi-and-moi-london?osq=Toi+%26+Moi+Caf%C3%A9",
                  "pages" : 3
              },
             {  "name"   : "The Breakfast Club",
                  "link"  : "https://www.yelp.com/biz/the-breakfast-club-london-2?osq=Toi+%26+Moi+Caf%C3%A9",
                  "pages" : 3
              }
             
]
#scraping function
def scrape(rest_list):
  all_comment_list = list()
  for rest in rest_list:
    comment_list = list()
    for pag in range(1, rest['pages']):
      try:
        time.sleep(5)
#URL = "https://www.yelp.com/biz/the-cortez-raleigh?osq=Restaurants&start="+str(pag*10)+"&sort_by=rating_asc"
        URL = rest['link']+str(pag*10)
        print(rest['name'], 'downloading page ', pag*10)
        page = requests.get(URL)
#next step: parsing
        soup = BeautifulSoup(page.content, 'lxml')
        soup
        for comm in soup.find("yelp-react-root").find_all("p", {"class" : "comment__373c0__Nsutg css-n6i4z7"}):
          comment_list.append(comm.find("span").decode_contents())
          print(comm.find("span").decode_contents())
      except:
        print("could not work properly!")
    all_comment_list.append([comment_list, rest['name']])
  return all_comment_list
#store all reviews in a list
reviews = scrape(rest_dict)

df = pd.DataFrame(reviews)
df.head(15)

df = df.explode(0)

df

df = df.reset_index(drop=True)
df[0:10]

def perform_sentiment(x):
  testimonial = TextBlob(x)
  #testimonial.sentiment (polarity, subjectvity)
  testimonial.sentiment.polarity
  #sentiment_list.append([sentence, testimonial.sentiment.polarity, testimonial.subjectivity])
  return testimonial.sentiment.polarity

!pip install spacy

def top_frequent(text, num_words):
  #frequency of most common words
  import spacy
  from collections import Counter
  nlp = spacy.load("en")
  text = text
  
  #lemmatization
  doc = nlp(text)
  token_list = list()
  for token in doc:
    #print(token, token.lemma_)
    token_list.append(token.lemma_)
  token_list
  lemmatized = ''
  for _ in token_list:
    lemmatized = lemmatized + ' ' + _
  lemmatized
#remove stopwords and punctuations
  doc = nlp(lemmatized)
  words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]
  word_freq = Counter(words)
  common_words = word_freq.most_common(num_words)
  return common_words

text = ' '.join(list(df[0].values[0:20]))
text
top_frequent(text, 20)